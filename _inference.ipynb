{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf3834cf-a7c3-46e0-99f4-4d664f49598c",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "This notebook contains code that can be used to run inferences of models. The inference results are stored in various file formats and later they are analyzed in other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f5d977-e04b-4a01-a439-5e4b318cea38",
   "metadata": {},
   "source": [
    "## Machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc2d6ea-9460-45d8-aad9-e3d7dac2292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_enabled_translator(translator_class, target_language, enabled=True):\n",
    "    if translator_class == AmazonTranslate:\n",
    "        return AmazonTranslate(\n",
    "            target_language=target_language,\n",
    "            enable_api=enabled,\n",
    "        ).load()\n",
    "\n",
    "    if translator_class == DeepL:\n",
    "        return DeepL(\n",
    "            target_language=target_language,\n",
    "            enable_api=enabled,\n",
    "            server_url='https://api.deepl.com/',\n",
    "        ).load()\n",
    "\n",
    "    if translator_class == GoogleTranslate:\n",
    "        return GoogleTranslate(\n",
    "            target_language=target_language,\n",
    "            enable_api=enabled,\n",
    "        ).load()\n",
    "\n",
    "    if translator_class == NLLB:\n",
    "        return NLLB(\n",
    "            target_language=target_language,\n",
    "            device='cuda:0',\n",
    "            enable_inference=enabled,\n",
    "        ).load()\n",
    "\n",
    "\n",
    "data = list()\n",
    "for translator_class in supported_languages.keys():\n",
    "    print(translator_class)\n",
    "    for language in supported_languages[translator_class]:\n",
    "        print(language)\n",
    "        translator = prepare_enabled_translator(translator_class, language, enabled=False)\n",
    "        preds = predictions(translator_class, language, lazy=True)\n",
    "        buf = []\n",
    "        for sen, ste, pred in zip(sentences, stereotypes, preds):\n",
    "\n",
    "            gender = None\n",
    "\n",
    "            if pred == 'male':\n",
    "                if translator_class == DeepL and language == 'cs':\n",
    "                    prompt = f'She said \"{sen}\"'\n",
    "                else:\n",
    "                    prompt = f'She said: \"{sen}\"'\n",
    "                # buf.append(prompt)\n",
    "                \n",
    "            if pred == 'female':\n",
    "                if translator_class == DeepL and language == 'cs':\n",
    "                    prompt = f'He said \"{sen}\"'\n",
    "                else:\n",
    "                    prompt = f'He said: \"{sen}\"'\n",
    "                # buf.append(prompt)\n",
    "\n",
    "            if pred:\n",
    "                translation = translator.translate([prompt])[prompt]\n",
    "                try:\n",
    "                    extracted = extract_sentence(sen, translation)\n",
    "                except:\n",
    "                    print('Extraction failed:', translation)\n",
    "                if extracted:\n",
    "                    original = translator.translate([sen])[sen]\n",
    "                    words_o, words_e = original.split(), extracted.split()\n",
    "                    if len(words_o) == len(words_e) and sum(wo != we for wo, we in zip(words_o, words_e)) == 1:\n",
    "                        if pred == 'male':\n",
    "                            male, female = original, extracted\n",
    "                        if pred == 'female':\n",
    "                            male, female = extracted, original\n",
    "                        data.append((\n",
    "                            translator_class.__name__,\n",
    "                            language,\n",
    "                            sen,\n",
    "                            ste,\n",
    "                            male,\n",
    "                            female,\n",
    "                        ))                \n",
    "\n",
    "            # if len(buf) == 250 or sen == sentences[-1]:\n",
    "            #     translation = translator.translate(buf, save=True)\n",
    "            #     for _from, _to in translation.items():\n",
    "            #         print(_from)\n",
    "            #         print(_to)\n",
    "            #         print(unidecode(_to))\n",
    "            #         print()\n",
    "            #         break\n",
    "            #     buf = []\n",
    "        del translator            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9a268-440d-4580-9e7e-5e2693b7f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "from parser import Parser\n",
    "\n",
    "import logging\n",
    "logging.getLogger('deepl').setLevel(logging.WARNING)\n",
    "\n",
    "for translator_class in translator_classes:\n",
    "    for target_language in supported_languages[translator_class]:\n",
    "        translator = translator_class(\n",
    "            # dir_path='./cache/translations/nllb_3b',\n",
    "            target_language=target_language,\n",
    "            # enable_api=True,\n",
    "            # server_url='https://api.deepl.com/',\n",
    "        ).load()\n",
    "        parser = Parser(language=target_language).load_model()\n",
    "        for x in range(len(sentences)//100 + 1):   \n",
    "            translations = translator.translate(sentences[x*100: x*100 + 100], save=True)  # Will be saved in translator dir\n",
    "            parser.parse(list(translations.values()))  # Will be saved in parser dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b7b2c-c04c-4858-ba4e-a5dc897e8539",
   "metadata": {},
   "source": [
    "## English MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0352d-124b-49e6-980a-5231193f1ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\n",
    "    (lambda s: f'He said: \"{s}\"', lambda s: f'She said: \"{s}\"'),\n",
    "    (lambda s: f'The man said: \"{s}\"', lambda s: f'The woman said: \"{s}\"'),\n",
    "    (lambda s: f'\"{s}\", he said.', lambda s: f'\"{s}\", she said.'),\n",
    "    (lambda s: f'\"{s}\", the man said.', lambda s: f'\"{s}\", the woman said.'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd0617-053c-4a38-be53-0e932fa2be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_mlm_score(sample, templates, model, tokenizer, device):\n",
    "    sen1, sen2 = templates[0](sample), templates[1](sample)\n",
    "    a = calculate_logprob(sen1, sen2, tokenizer, model, device=device)\n",
    "    b = calculate_logprob(sen2, sen1, tokenizer, model, device=device)\n",
    "    return a - b\n",
    "\n",
    "def make_predictions(model_handle, samples, templates, device):\n",
    "    model, tokenizer = model_init(model_handle)\n",
    "    return [\n",
    "        english_mlm_score(sample, templates, model, tokenizer, device)\n",
    "        for sample in tqdm(samples)\n",
    "    ]\n",
    "\n",
    "models = [\n",
    "    'distilbert-base-uncased',\n",
    "    'google/electra-large-generator',\n",
    "    'google/electra-base-generator',\n",
    "]\n",
    "\n",
    "for model_handle in models:\n",
    "    for i, template in enumerate(templates):\n",
    "        preds = make_predictions(model_handle, sentences, template, 'cuda:0')\n",
    "        import os\n",
    "        os.makedirs('./data/predictions/english_mlm', exist_ok=True)\n",
    "        with open(f'./cache/predictions/english_mlm/{model_handle.split(\"/\")[-1]}_template-{i}.txt', 'w') as f:\n",
    "            f.write('\\n'.join(map(str, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9346ac91-9831-40c0-b49f-41d7647293ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://huggingface.co/api/models\"\n",
    "payload = {\"search\": \"google/multiberts\"}\n",
    "response = requests.get(url, params=payload)\n",
    "handles = [\n",
    "    hit['id']\n",
    "    for hit in response.json()\n",
    "]\n",
    "handles\n",
    "\n",
    "import os\n",
    "\n",
    "for handle in handles[5:]:\n",
    "    for t_id, template in enumerate(templates):\n",
    "        preds = make_predictions(handle, sentences, template, 'cuda:0')\n",
    "        dir_name = handle.split('/')[1]\n",
    "        os.makedirs('./cache/predictions/multibert', exist_ok=True)\n",
    "        with open(f'./cache/predictions/multibert/{dir_name}_template-{t_id}.txt', 'w') as f:\n",
    "            f.write('\\n'.join(map(str, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bca70b-d3b6-416d-88fe-09a8d6953a49",
   "metadata": {},
   "source": [
    "## Slavic MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ab223-6d7c-4cc9-aeb4-c04e8b08ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "patterns = [\n",
    "    r'\"(.+)\"',\n",
    "    r'„(.+)“',\n",
    "    r'„(.+)”',\n",
    "    r'“(.+)”',\n",
    "    r'«(.+)»',\n",
    "    r'»(.+)«',\n",
    "    r'„(.+)\"',\n",
    "    r'\"(.+)',\n",
    "    r'„(.+)',\n",
    "    r'„(.+)',\n",
    "    r'»(.+)',\n",
    "    r': (.+)',\n",
    "    r'(.+)',\n",
    "]\n",
    "\n",
    "def extract_sentence(original, translation):\n",
    "    if any((re_lst := re.findall(pattern, translation)) for pattern in patterns):\n",
    "        extracted = re_lst[0]\n",
    "        if original[-1] in '.?!' and extracted[-1] not in '.?!':\n",
    "            extracted += o[-1]\n",
    "        return extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fafec78-e5a8-4ee3-b661-f694f2d944c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "df = pd.DataFrame(data, columns=['translator', 'language', 'original', 'stereotype', 'male', 'female'])\n",
    "df.to_csv('./data/gender_variants.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe4000-c6fc-4c47-96a1-eeee15eb8172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "male_buf, female_buf = list(), list()\n",
    "for lang in ['be', 'ru', 'uk', 'hr', 'sl', 'sr', 'cs', 'pl', 'sk']:\n",
    "    if translator_class in translator_classes:\n",
    "        \n",
    "        if lang not in supported_languages[translator_class]:\n",
    "            continue\n",
    "\n",
    "        translator = translator_class(target_language=lang).load()\n",
    "\n",
    "\n",
    "        male_buf.extend([\n",
    "            translator.dataframe.loc[sen]['to']\n",
    "            for sen, pred in zip(sentences, predictions(translator_class, lang, lazy=True))\n",
    "            if pred == 'male'\n",
    "        ])\n",
    "        female_buf.extend([\n",
    "            translator.dataframe.loc[sen]['to']\n",
    "            for sen, pred in zip(sentences, predictions(translator_class, lang, lazy=True))\n",
    "            if pred == 'female'\n",
    "        ])\n",
    "\n",
    "for s in random.sample(male_buf, 50):\n",
    "    print(s)\n",
    "    print(unidecode(s))\n",
    "print('=====')-\n",
    "for s in random.sample(female_buf, 50):\n",
    "    print(s)\n",
    "    print(unidecode(s))\n",
    "print('=====')\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a579f7c-6568-473e-afbe-1de04ae6104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_score(sample, model, tokenizer, device):\n",
    "    sen1, sen2 = sample\n",
    "    a = calculate_logprob(sen1, sen2, tokenizer, model, device=device)\n",
    "    b = calculate_logprob(sen2, sen1, tokenizer, model, device=device)\n",
    "    return a - b\n",
    "\n",
    "def make_predictions(model_handle, samples, device):\n",
    "    model, tokenizer = model_init(model_handle)\n",
    "    return [\n",
    "        mlm_score(sample, model, tokenizer, device)\n",
    "        for sample in tqdm(samples)\n",
    "    ]\n",
    "    \n",
    "for model_handle in models:\n",
    "    preds = make_predictions(model_handle, list(zip(df.male, df.female)), 'cuda:0')\n",
    "    import os\n",
    "    os.makedirs('./cache/predictions/mmlm', exist_ok=True)\n",
    "    with open(f'./cache/predictions/mmlm/{model_handle.split(\"/\")[-1]}.txt', 'w') as f:\n",
    "        f.write('\\n'.join(map(str, preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
