{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf3834cf-a7c3-46e0-99f4-4d664f49598c",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "This notebook contains code that can be used to run inferences of models. The inference results are stored in various file formats and later they are analyzed in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4566de51-888d-4195-997d-248966b86640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gest import gest\n",
    "from masked_models.utils import model_init, calculate_logprob\n",
    "from parser import Parser\n",
    "\n",
    "\n",
    "from translators.google_translate import GoogleTranslate\n",
    "from translators.amazon_translate import AmazonTranslate\n",
    "from translators.deepl import DeepL\n",
    "from translators.nllb import NLLB\n",
    "translator_classes = AmazonTranslate, DeepL, GoogleTranslate, NLLB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f5d977-e04b-4a01-a439-5e4b318cea38",
   "metadata": {},
   "source": [
    "## Machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc2d6ea-9460-45d8-aad9-e3d7dac2292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_enabled_translator(translator_class, target_language, enabled=True):\n",
    "    if translator_class == AmazonTranslate:\n",
    "        return AmazonTranslate(\n",
    "            target_language=target_language,\n",
    "            enable_api=enabled,\n",
    "        ).load()\n",
    "\n",
    "    if translator_class == DeepL:\n",
    "        return DeepL(\n",
    "            target_language=target_language,\n",
    "            enable_api=enabled,\n",
    "            server_url='https://api.deepl.com/',\n",
    "        ).load()\n",
    "\n",
    "    if translator_class == GoogleTranslate:\n",
    "        return GoogleTranslate(\n",
    "            target_language=target_language,\n",
    "            enable_api=enabled,\n",
    "        ).load()\n",
    "\n",
    "    if translator_class == NLLB:\n",
    "        return NLLB(\n",
    "            target_language=target_language,\n",
    "            device='cuda:0',\n",
    "            enable_inference=enabled,\n",
    "        ).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9a268-440d-4580-9e7e-5e2693b7f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('deepl').setLevel(logging.WARNING)\n",
    "\n",
    "for translator_class in translator_classes:\n",
    "    for target_language in translator_class.supported_languages:\n",
    "        translator = prepare_enabled_translator(translator_class, target_language)\n",
    "        parser = Parser(language=target_language).load_model()\n",
    "        for x in range(len(gest.sentence)//100 + 1):   \n",
    "            translations = translator.translate(gest.sentence[x*100: x*100 + 100], save=True)  # Will be saved in translator dir\n",
    "            parser.parse(list(translations.values()))  # Will be saved in parser dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b7b2c-c04c-4858-ba4e-a5dc897e8539",
   "metadata": {},
   "source": [
    "## English MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0352d-124b-49e6-980a-5231193f1ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_mlm_score(sample, templates, model, tokenizer, device):\n",
    "    sen1, sen2 = templates[0](sample), templates[1](sample)\n",
    "    a = calculate_logprob(sen1, sen2, tokenizer, model, device=device)\n",
    "    b = calculate_logprob(sen2, sen1, tokenizer, model, device=device)\n",
    "    return a - b\n",
    "\n",
    "def make_predictions(model_handle, samples, templates, device):\n",
    "    model, tokenizer = model_init(model_handle)\n",
    "    return [\n",
    "        english_mlm_score(sample, templates, model, tokenizer, device)\n",
    "        for sample in tqdm(samples)\n",
    "    ]\n",
    "\n",
    "models = [\n",
    "    'bert-base-uncased',\n",
    "    'roberta-base',    \n",
    "    'albert-base-v2',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'xlm-roberta-base',\n",
    "    'xlm-roberta-large',\n",
    "    'facebook/xlm-v-base',\n",
    "    'facebook/xlm-roberta-xl',\n",
    "    'distilbert-base-uncased',\n",
    "    'google/electra-large-generator',\n",
    "    'google/electra-base-generator',\n",
    "]\n",
    "\n",
    "templates = [\n",
    "    (lambda s: f'He said: \"{s}\"', lambda s: f'She said: \"{s}\"'),\n",
    "    (lambda s: f'The man said: \"{s}\"', lambda s: f'The woman said: \"{s}\"'),\n",
    "    (lambda s: f'\"{s}\", he said.', lambda s: f'\"{s}\", she said.'),\n",
    "    (lambda s: f'\"{s}\", the man said.', lambda s: f'\"{s}\", the woman said.'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd0617-053c-4a38-be53-0e932fa2be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Common MLMs reported in the paper\n",
    "\"\"\"\n",
    "\n",
    "for model_handle in models:\n",
    "    for i, template in enumerate(templates):\n",
    "        preds = make_predictions(model_handle, gest.sentence, template, 'cuda:0')\n",
    "        os.makedirs('./data/predictions/english_mlm', exist_ok=True)\n",
    "        with open(f'./cache/predictions/english_mlm/{model_handle.split(\"/\")[-1]}_template-{i}.txt', 'w') as f:\n",
    "            f.write('\\n'.join(map(str, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9346ac91-9831-40c0-b49f-41d7647293ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MultiBERT checkpoints\n",
    "\"\"\"\n",
    "\n",
    "url = 'https://huggingface.co/api/models'\n",
    "payload = {'search': 'google/multiberts'}\n",
    "response = requests.get(url, params=payload)\n",
    "handles = [\n",
    "    hit['id']\n",
    "    for hit in response.json()\n",
    "]\n",
    "\n",
    "for handle in handles:\n",
    "    for t_id, template in enumerate(templates):\n",
    "        preds = make_predictions(handle, gest.sentence, template, 'cuda:0')\n",
    "        dir_name = handle.split('/')[1]\n",
    "        os.makedirs('./cache/predictions/multibert', exist_ok=True)\n",
    "        with open(f'./cache/predictions/multibert/{dir_name}_template-{t_id}.txt', 'w') as f:\n",
    "            f.write('\\n'.join(map(str, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bca70b-d3b6-416d-88fe-09a8d6953a49",
   "metadata": {},
   "source": [
    "## Slavic MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681eb032-1459-4f27-b151-89e3c456b4e7",
   "metadata": {},
   "source": [
    "### Creating `gender_variants.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf59ea4-0848-4b0b-b87c-d7799879246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    r'\"(.+)\"',\n",
    "    r'„(.+)“',\n",
    "    r'„(.+)”',\n",
    "    r'“(.+)”',\n",
    "    r'«(.+)»',\n",
    "    r'»(.+)«',\n",
    "    r'„(.+)\"',\n",
    "    r'\"(.+)',\n",
    "    r'„(.+)',\n",
    "    r'„(.+)',\n",
    "    r'»(.+)',\n",
    "    r': (.+)',\n",
    "    r'(.+)',\n",
    "]\n",
    "\n",
    "def extract_sentence(original, translation):\n",
    "    \"\"\"\n",
    "    Extract only the core sentence from a translation that contains a translated template as well (e.g., He said:)\n",
    "    \"\"\"\n",
    "    if any((re_lst := re.findall(pattern, translation)) for pattern in patterns):\n",
    "        extracted = re_lst[0]\n",
    "        if original[-1] in '.?!' and extracted[-1] not in '.?!':  # Interpunction fix\n",
    "            extracted += o[-1]\n",
    "        return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327200c5-385e-42c2-a102-30b576b7e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "\n",
    "for translator_class in translator_classes:\n",
    "    for language in translator_class.supported_languages:\n",
    "        translator = prepare_enabled_translator(translator_class, language, enabled=False)  # We already assume that the translations were made elsewhere\n",
    "        preds = predictions(translator_class, language, lazy=True)\n",
    "\n",
    "        for sentence, stereotype_id, predicted_gender in zip(gest.sentence, gest.stereotype, preds):\n",
    "\n",
    "            if predicted_gender == 'male':\n",
    "                if translator_class == DeepL and language == 'cs': # DeepL has serious issues with `:` in source strings in Czech.\n",
    "                    prompt = f'She said \"{sentence}\"'\n",
    "                else:\n",
    "                    prompt = f'She said: \"{sentence}\"'  \n",
    "                \n",
    "            if predicted_gender == 'female':\n",
    "                if translator_class == DeepL and language == 'cs': # DeepL has serious issues with `:` in source strings in Czech.\n",
    "                    prompt = f'He said \"{sentence}\"'\n",
    "                else:\n",
    "                    prompt = f'He said: \"{sentence}\"'\n",
    "\n",
    "            if predicted_gender:\n",
    "                translation = translator.translate([prompt])[prompt]\n",
    "                \n",
    "                try:\n",
    "                    extracted = extract_sentence(sentence, translation)\n",
    "                except:\n",
    "                    print('Extraction failed:', translation)\n",
    "                    \n",
    "                if extracted:\n",
    "                    original = translator.translate([sentence])[sentence]\n",
    "                    words_o, words_e = original.split(), extracted.split()\n",
    "                    if len(words_o) == len(words_e) and sum(wo != we for wo, we in zip(words_o, words_e)) == 1:\n",
    "                        if predicted_gender == 'male':\n",
    "                            male, female = original, extracted\n",
    "                        if predicted_gender == 'female':\n",
    "                            male, female = extracted, original\n",
    "                        data.append((\n",
    "                            translator_class.__name__,\n",
    "                            language,\n",
    "                            sentence,\n",
    "                            stereotype_id,\n",
    "                            male,\n",
    "                            female,\n",
    "                        ))                \n",
    "        del translator   \n",
    "\n",
    "\n",
    "df = pd.DataFrame(data, columns=['translator', 'language', 'original', 'stereotype', 'male', 'female'])\n",
    "df.to_csv('./data/gender_variants.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f795075-b4b6-4c15-81de-a0e21b383476",
   "metadata": {},
   "source": [
    "### Calculating MLM scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ff593-dad7-4aa8-ac7f-ce729a6d42d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'bert-base-multilingual-cased',\n",
    "    'xlm-roberta-base',\n",
    "    'xlm-roberta-large',\n",
    "    'facebook/xlm-v-base',\n",
    "    'facebook/xlm-roberta-xl',\n",
    "]\n",
    "\n",
    "def slavic_mlm_score(sample, model, tokenizer, device):\n",
    "    sen1, sen2 = sample\n",
    "    a = calculate_logprob(sen1, sen2, tokenizer, model, device=device)\n",
    "    b = calculate_logprob(sen2, sen1, tokenizer, model, device=device)\n",
    "    return a - b\n",
    "\n",
    "def make_predictions(model_handle, samples, device):\n",
    "    model, tokenizer = model_init(model_handle)\n",
    "    return [\n",
    "        slavic_mlm_score(sample, model, tokenizer, device)\n",
    "        for sample in tqdm(samples)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a579f7c-6568-473e-afbe-1de04ae6104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/gender_variants.csv')\n",
    "\n",
    "for model_handle in models:\n",
    "    preds = make_predictions(model_handle, list(zip(df.male, df.female)), 'cuda:0')\n",
    "    os.makedirs('./data/predictions/slavic_mlm', exist_ok=True)\n",
    "    with open(f'./data/predictions/slavic_mlm/{model_handle.split(\"/\")[-1]}.txt', 'w') as f:\n",
    "        f.write('\\n'.join(map(str, preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
